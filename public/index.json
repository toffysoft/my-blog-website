[{"content":"ลองเล่น Local LLM ด้วย Ollama + Python บทความนี้จะพาทุกคนมาลองใช้งาน LLM บนเครื่องคอมพิวเตอร์ส่วนตัวผ่าน Ollama และ Python เหมาะสำหรับผู้ที่อยากทดลองเล่น AI แต่กังวลเรื่องความเป็นส่วนตัวของข้อมูล หรือต้องการระบบที่ทำงานได้แม้ไม่มีอินเทอร์เน็ต\nที่มาของ Large Language Model (LLM) ในช่วงไม่กี่ปีที่ผ่านมา เราได้เห็นการเติบโตอย่างก้าวกระโดดของ AI โดยเฉพาะในด้านการประมวลผลภาษาธรรมชาติ จุดเปลี่ยนสำคัญเกิดขึ้นเมื่อนักวิจัยพบว่า การสร้างโมเดลขนาดใหญ่และฝึกฝนด้วยข้อมูลมหาศาล ทำให้ AI สามารถเข้าใจและตอบโต้กับมนุษย์ได้อย่างน่าทึ่ง\nปัจจุบันมีบริการ LLM มากมายให้เลือกใช้ เช่น ChatGPT, Claude, Gemini แต่หลายคนอาจกังวลเรื่องความเป็นส่วนตัวของข้อมูล หรือต้องการระบบที่ทำงานได้แม้ไม่มีอินเทอร์เน็ต นั่นคือที่มาของ Local LLM\nรู้จักกับ Ollama Ollama เป็นเครื่องมือที่ช่วยให้เราสามารถรัน LLM บนเครื่องคอมพิวเตอร์ส่วนตัวได้อย่างง่ายดาย รองรับโมเดลหลากหลาย เช่น Llama 3, Mistral, CodeLlama โดยมีจุดเด่นคือ:\nติดตั้งง่าย รองรับทั้ง Windows, macOS และ Linux มี API ที่ใช้งานสะดวก ประสิทธิภาพดี ใช้ทรัพยากรเครื่องน้อย รองรับการปรับแต่งโมเดลได้ตามต้องการ การติดตั้ง 1. ติดตั้ง Ollama สำหรับ macOS:\nbrew install ollama สำหรับ Linux:\ncurl -fsSL https://ollama.com/install.sh | sh สำหรับ Windows สามารถดาวน์โหลดได้จาก เว็บไซต์ Ollama\n2. ติดตั้ง Python Package pip install ollama เริ่มต้นใช้งาน 1. ดาวน์โหลดโมเดล เริ่มจากเปิด Terminal แล้วรันคำสั่ง:\nollama pull llama3.1 2. ทดสอบด้วย Python สร้างไฟล์ test_ollama.py:\nimport ollama def simple_chat(): response = ollama.chat(model=\u0026#39;llama3.1\u0026#39;, messages=[ {\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;สวัสดี คุณทำอะไรได้บ้าง?\u0026#39;} ]) print(response[\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;]) # ทดสอบเรียกใช้งาน if __name__ == \u0026#39;__main__\u0026#39;: simple_chat() ลองรันทดสอบ:\npython test_ollama.py Output:\nสวัสดีค่ะ ฉันสามารถตอบคำถามของคุณได้ เช่น การเรียนรู้ภาษา คำนวณเลขคณิต ช่วยหาข้อมูลเกี่ยวกับประเทศหรือเมือง ขอข้อมูลเกี่ยวกับต่างๆ อีกมากมายค่ะ การใช้งานขั้นสูงขึ้น การสร้าง Chat Assistant สร้างไฟล์ assistant.py:\nimport ollama from typing import List, Dict class ChatAssistant: def __init__(self, model_name: str = \u0026#39;llama3.1\u0026#39;): self.model = model_name self.conversation_history: List[Dict[str, str]] = [] def chat(self, message: str) -\u0026gt; str: self.conversation_history.append({ \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: message }) response = ollama.chat( model=self.model, messages=self.conversation_history ) self.conversation_history.append({ \u0026#39;role\u0026#39;: \u0026#39;assistant\u0026#39;, \u0026#39;content\u0026#39;: response[\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;] }) return response[\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;] def clear_history(self): self.conversation_history = [] ตัวอย่างการใช้งาน Chat Assistant สร้างไฟล์ chat.py:\nfrom assistant import ChatAssistant assistant = ChatAssistant() questions = [ \u0026#34;Python คืออะไร?\u0026#34;, \u0026#34;ยกตัวอย่างการใช้งาน list comprehension\u0026#34;, \u0026#34;แล้ว dictionary comprehension ล่ะ?\u0026#34; ] for question in questions: print(f\u0026#34;\\nคำถาม: {question}\u0026#34;) print(f\u0026#34;คำตอบ: {assistant.chat(question)}\u0026#34;) ลองรันทดสอบ:\npython chat.py Output:\nคำถาม: Python คืออะไร? คำตอบ: ภาษาเชิงสคริปต์ (Scripting language) ที่ใช้ในการเขียนโปรแกรมคอมพิวเตอร์ โดยมีลักษณะเฉพาะคือความสามารถในการนำโค้ดไปใช้งานได้ทันทีโดยไม่ต้องบันทึกลงไปในไฟล์ใดๆ คำถาม: ยกตัวอย่างการใช้งาน list comprehension คำตอบ: **List Comprehension ในภาษา Python** List comprehension เป็นฟังก์ชันพิเศษในภาษา Python ที่สามารถสร้างรายการ (list) ได้อย่างรวดเร็วและง่ายดาย โดยไม่ต้องใช้ loop หรือการเขียนโค้ดซ้ำๆ ตัวอย่างการใช้งาน list comprehension: **1. สร้างรายการที่มีขนาดเฉพาะ** `python numbers = [i for i in range(10)] print(numbers) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] ` **2. ฟิลเตอร์รายการ** `python numbers = [i for i in range(10) if i % 2 == 0] print(numbers) # [0, 2, 4, 6, 8] ` **3. ทำการปฏิบัติการบนรายการ** `python numbers = [i ** 2 for i in range(5)] print(numbers) # [0, 1, 4, 9, 16] ` **4. รวมสองรายการเข้าด้วยกัน** `python names = [\u0026#39;John\u0026#39;, \u0026#39;Alice\u0026#39;, \u0026#39;Bob\u0026#39;] ages = [25, 30, 35] people = [{name: age} for name, age in zip(names, ages)] print(people) # [{\u0026#39;John\u0026#39;: 25}, {\u0026#39;Alice\u0026#39;: 30}, {\u0026#39;Bob\u0026#39;: 35}] ` นี่คือตัวอย่างการใช้งาน list comprehension ในภาษา Python มีหลายกรณีที่สามารถใช้ได้ และมันช่วยให้คุณเขียนโค้ดที่กระชับและง่ายดายมากขึ้น! คำถาม: แล้ว dictionary comprehension ล่ะ? คำตอบ: **Dictionary Comprehension ในภาษา Python** Dictionary comprehension เป็นฟังก์ชันพิเศษในภาษา Python ที่สามารถสร้าง辞านวารี (dictionary) ได้อย่างรวดเร็วและง่ายดาย โดยไม่ต้องใช้ loop หรือการเขียนโค้ดซ้ำๆ ตัวอย่างการใช้งาน dictionary comprehension: **1. สร้าง辞านวารีที่มีขนาดเฉพาะ** `python numbers = {i: i * 2 for i in range(5)} print(numbers) # {0: 0, 1: 2, 2: 4, 3: 6, 4: 8} ` **2. ฟิลเตอร์รายการ** `python numbers = {i: i * 2 for i in range(10) if i % 2 == 0} print(numbers) # {0: 0, 2: 4, 4: 8, 6: 12, 8: 16} ` **3. ทำการปฏิบัติการบนรายการ** `python numbers = {i: i ** 2 for i in range(5)} print(numbers) # {0: 0, 1: 1, 2: 4, 3: 9, 4: 16} ` **4. รวมสองรายการเข้าด้วยกัน** `python names = [\u0026#39;John\u0026#39;, \u0026#39;Alice\u0026#39;, \u0026#39;Bob\u0026#39;] ages = [25, 30, 35] people = {name: age for name, age in zip(names, ages)} print(people) # {\u0026#39;John\u0026#39;: 25, \u0026#39;Alice\u0026#39;: 30, \u0026#39;Bob\u0026#39;: 35} ` นี่คือตัวอย่างการใช้งาน dictionary comprehension ในภาษา Python มีหลายกรณีที่สามารถใช้ได้ และมันช่วยให้คุณเขียนโค้ดที่กระชับและง่ายดายมากขึ้น! ความแตกต่างระหว่าง list comprehension และ dictionary comprehension คือ: * List comprehension สร้างรายการ (list) ขณะที่ dictionary comprehension สร้าง辞านวารี (dictionary) * ใน list comprehension เราสามารถใช้คำสั่ง `for` ได้ทั้งสองฝ่าย (left-hand side และ right-hand side) ในขณะที่ใน dictionary comprehension เราสามารถใช้คำสั่ง `for` ได้เพียงฝ่ายหนึ่งเท่านั้น การปรับแต่งพารามิเตอร์ เราสามารถปรับแต่งการทำงานของ LLM ได้ผ่านพารามิเตอร์ต่างๆ:\nสร้างไฟล์ advanced_chat.py:\nimport ollama def advanced_chat(prompt: str): response = ollama.chat( model=\u0026#39;llama3.1\u0026#39;, messages=[{\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: prompt}], options={ \u0026#39;temperature\u0026#39;: 0.7, # ควบคุมความสร้างสรรค์ (0.0 - 1.0) \u0026#39;top_p\u0026#39;: 0.9, # ควบคุมความหลากหลายของคำตอบ \u0026#39;top_k\u0026#39;: 40, # จำนวนโทเค็นที่พิจารณา \u0026#39;num_predict\u0026#39;: 4069 # ความยาวสูงสุดของคำตอบ } ) return response[\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;] # ทดสอบเรียกใช้งาน if __name__ == \u0026#39;__main__\u0026#39;: prompt = \u0026#34;เล่าเรื่องตลกให้ฟังหน่อยสิ\u0026#34; print(advanced_chat(prompt)) ลองรันทดสอบ:\npython advanced_chat.py Output:\nมีชายคนหนึ่งซื้อหมูจากตลาดกลับบ้านเพื่อให้ทานเย็น แต่เมื่อลูกสาวของเขาเห็นหมู เธอก็บอกพ่อว่า \u0026#34;พ่อ ฉันอยากจะเลี้ยงหมูตัวนั้นก่อน\u0026#34; ชายคนนั้นพยายามที่จะทำให้ลูกสาวตกใจและบอกเธอว่า \u0026#34;หมูนี้เป็นหมูที่มีชื่อเสียงมาก มันสามารถปรุงแต่งอาหารได้ทุกชนิด แต่สิ่งที่สำคัญที่สุดคือมันไม่ต้องการเงิน\u0026#34; หญิงสาวตอบว่า \u0026#34;นั่นก็ทำให้ฉันประหลาดใจจริงๆ ที่เราสามารถจ่ายค่าตอบแทนทางเงินให้มันได้!\u0026#34; การใช้งานกับ Stream Ollama รองรับการ stream ข้อความตอบกลับแบบ real-time:\nimport ollama def stream_chat(prompt: str): stream = ollama.chat( model=\u0026#39;llama3.1\u0026#39;, messages=[{\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: prompt}], stream=True ) # พิมพ์ข้อความทีละส่วนตามที่ได้รับ for chunk in stream: if chunk[\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;]: print(chunk[\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;], end=\u0026#39;\u0026#39;, flush=True) การจัดการกับข้อผิดพลาด import ollama def safe_chat(prompt: str) -\u0026gt; str: try: response = ollama.chat( model=\u0026#39;llama3.1\u0026#39;, messages=[{\u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: prompt}] ) return response[\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;] except Exception as e: return f\u0026#34;เกิดข้อผิดพลาด: {str(e)}\u0026#34; ข้อควรระวังและข้อจำกัด ทรัพยากรเครื่อง\nต้องการ RAM อย่างน้อย 8GB ควรมี GPU สำหรับประสิทธิภาพที่ดี พื้นที่ดิสก์สำหรับเก็บโมเดล (ประมาณ 4-8GB ต่อโมเดล) ความแม่นยำ\nLocal LLM อาจมีความแม่นยำน้อยกว่าโมเดลออนไลน์ ควรตรวจสอบผลลัพธ์เสมอ โดยเฉพาะในงานสำคัญ การอัพเดท\nติดตามการอัพเดทของ Ollama และโมเดลอยู่เสมอ อาจต้อง pull โมเดลใหม่เมื่อมีเวอร์ชันอัพเดท สรุป การใช้ Local LLM ผ่าน Ollama เป็นทางเลือกที่น่าสนใจสำหรับผู้ที่ต้องการความเป็นส่วนตัวหรือต้องการระบบที่ทำงานได้แบบ offline ถึงแม้จะมีข้อจำกัดบางประการ แต่ก็สามารถนำไปประยุกต์ใช้ได้หลากหลาย ตั้งแต่การสร้าง chatbot ไปจนถึงการประมวลผลเอกสาร\nแหล่งข้อมูลเพิ่มเติม GitHub Repo Ollama Official Documentation Ollama GitHub Repository Python Package Documentation บทความนี้อัพเดทล่าสุด: กุมภาพันธ์ 2025\nNote: ตัวอย่างโค้ดทั้งหมดทดสอบบน Python 3.10+\nCover image by Ollama\nปล. บทความนี้เขียนด้วย AI (^ . ^)\n","permalink":"http://localhost:1313/posts/ollama-python/","summary":"\u003ch1 id=\"ลองเลน-local-llm-ดวย-ollama--python\"\u003eลองเล่น Local LLM ด้วย Ollama + Python\u003c/h1\u003e\n\u003cp\u003e\u003cem\u003eบทความนี้จะพาทุกคนมาลองใช้งาน LLM บนเครื่องคอมพิวเตอร์ส่วนตัวผ่าน Ollama และ Python เหมาะสำหรับผู้ที่อยากทดลองเล่น AI แต่กังวลเรื่องความเป็นส่วนตัวของข้อมูล หรือต้องการระบบที่ทำงานได้แม้ไม่มีอินเทอร์เน็ต\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"ทมาของ-large-language-model-llm\"\u003eที่มาของ Large Language Model (LLM)\u003c/h2\u003e\n\u003cp\u003eในช่วงไม่กี่ปีที่ผ่านมา เราได้เห็นการเติบโตอย่างก้าวกระโดดของ AI โดยเฉพาะในด้านการประมวลผลภาษาธรรมชาติ จุดเปลี่ยนสำคัญเกิดขึ้นเมื่อนักวิจัยพบว่า การสร้างโมเดลขนาดใหญ่และฝึกฝนด้วยข้อมูลมหาศาล ทำให้ AI สามารถเข้าใจและตอบโต้กับมนุษย์ได้อย่างน่าทึ่ง\u003c/p\u003e\n\u003cp\u003eปัจจุบันมีบริการ LLM มากมายให้เลือกใช้ เช่น ChatGPT, Claude, Gemini แต่หลายคนอาจกังวลเรื่องความเป็นส่วนตัวของข้อมูล หรือต้องการระบบที่ทำงานได้แม้ไม่มีอินเทอร์เน็ต นั่นคือที่มาของ Local LLM\u003c/p\u003e\n\u003ch2 id=\"รจกกบ-ollama\"\u003eรู้จักกับ Ollama\u003c/h2\u003e\n\u003cp\u003eOllama เป็นเครื่องมือที่ช่วยให้เราสามารถรัน LLM บนเครื่องคอมพิวเตอร์ส่วนตัวได้อย่างง่ายดาย รองรับโมเดลหลากหลาย เช่น Llama 3, Mistral, CodeLlama โดยมีจุดเด่นคือ:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eติดตั้งง่าย รองรับทั้ง Windows, macOS และ Linux\u003c/li\u003e\n\u003cli\u003eมี API ที่ใช้งานสะดวก\u003c/li\u003e\n\u003cli\u003eประสิทธิภาพดี ใช้ทรัพยากรเครื่องน้อย\u003c/li\u003e\n\u003cli\u003eรองรับการปรับแต่งโมเดลได้ตามต้องการ\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"การตดตง\"\u003eการติดตั้ง\u003c/h2\u003e\n\u003ch3 id=\"1-ตดตง-ollama\"\u003e1. ติดตั้ง Ollama\u003c/h3\u003e\n\u003cp\u003eสำหรับ macOS:\u003c/p\u003e","title":"ลองเล่น Local LLM ด้วย Ollama + Python"}]